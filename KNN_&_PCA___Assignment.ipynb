{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## KNN & PCA | Assignment\n"
      ],
      "metadata": {
        "id": "Ya6OC2LeTmAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems? Answer: K-Nearest Neighbors (KNN) is a non-parametric, lazy learning algorithm used for both classification and regression.\n",
        "\n",
        "Classification: KNN identifies the 'k' closest data points to a new input. The new point is assigned the class that is most common (majority vote) among its neighbors.\n",
        "\n",
        "\n",
        "Regression: Instead of a majority vote, KNN takes the average (mean) of the values of the 'k' nearest neighbors to predict the value for the new input."
      ],
      "metadata": {
        "id": "CxH_DcJnT7o6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: What is the Curse of Dimensionality and how does it affect KNN performance? Answer: The \"Curse of Dimensionality\" refers to the problem that arises when the number of features (dimensions) in a dataset increases.\n",
        "\n",
        "Effect on KNN: KNN relies on distance (like Euclidean distance) to find neighbors. In high-dimensional space, the distance between all points becomes almost equal, making it hard for KNN to distinguish between \"near\" and \"far\" neighbors. This leads to a significant drop in model performance and increased computational time."
      ],
      "metadata": {
        "id": "CwDovFqjUAso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: What is Principal Component Analysis (PCA)? How is it different from feature selection? Answer: PCA is a dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the original information.\n",
        "\n",
        "\n",
        "Difference: * Feature Selection: It keeps some features and completely removes others (e.g., keeping \"Height\" and deleting \"Age\").\n",
        "\n",
        "PCA: It creates entirely new variables (Principal Components) by combining the existing ones. No original feature is simply \"deleted\"; they are all compressed into new ones."
      ],
      "metadata": {
        "id": "m-RFkRb_UBsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: What are eigenvalues and eigenvectors in PCA, and why are they important?  Answer:\n",
        "\n",
        "Eigenvectors: These determine the direction of the new feature space (Principal Components). They show how the data is oriented.\n",
        "\n",
        "\n",
        "Eigenvalues: These determine the magnitude or the amount of variance explained by each eigenvector.\n",
        "\n",
        "\n",
        "Importance: They are critical because they help identify which directions (Principal Components) capture the most information, allowing us to reduce dimensions while keeping the most important data."
      ],
      "metadata": {
        "id": "nFlXZumsUId_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: How do KNN and PCA complement each other when applied in a single pipeline? Answer: KNN and PCA work together perfectly to solve the \"Curse of Dimensionality.\"\n",
        "\n",
        "\n",
        "PCA acts as a pre-processing step to reduce the number of noisy or redundant features and compress the data.\n",
        "\n",
        "\n",
        "KNN then works on this reduced data, which makes the distance calculations much more accurate and the overall algorithm significantly faster."
      ],
      "metadata": {
        "id": "KRdhrDq2UQ0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases. Answer:"
      ],
      "metadata": {
        "id": "Ug6zpFswUipF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Data load aur split\n",
        "data = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Without Scaling\n",
        "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
        "acc_no_scale = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "# 2. With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "knn_scaled = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scale:.4f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcmyJtnCUlHO",
        "outputId": "ffd97fe3-76b1-44cb-ec32-6c2ce54d9eda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407\n",
            "Accuracy with scaling: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component. Answer:"
      ],
      "metadata": {
        "id": "HAWwLkvgUrPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# PCA ke liye scaling zaroori hai\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data.data)\n",
        "\n",
        "# PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr6QOBvOUss_",
        "outputId": "c1196a90-fe04-4f7c-c254-450a85670daa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset. Answer:"
      ],
      "metadata": {
        "id": "S7VqaR0xU84H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA with 2 components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_train_pca = pca_2.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_2.transform(X_test_scaled)\n",
        "\n",
        "# KNN on PCA data\n",
        "knn_pca = KNeighborsClassifier().fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "\n",
        "print(f\"Original Scaled Accuracy: {acc_scaled:.4f}\")\n",
        "print(f\"PCA (2 components) Accuracy: {acc_pca:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXrU-JKmU_ET",
        "outputId": "5d9c5ea0-142d-4431-b9dc-5204c53ab61b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Scaled Accuracy: 0.9630\n",
            "PCA (2 components) Accuracy: 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results. Answer:"
      ],
      "metadata": {
        "id": "uUpsS6DoVDSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn_metric = KNeighborsClassifier(metric=metric).fit(X_train_scaled, y_train)\n",
        "    acc = accuracy_score(y_test, knn_metric.predict(X_test_scaled))\n",
        "    print(f\"Accuracy with {metric} distance: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY9eniX5VIru",
        "outputId": "500fd510-d746-4ee4-f3c8-3d6d68e4cf70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with euclidean distance: 0.9630\n",
            "Accuracy with manhattan distance: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Explain how you would use PCA and KNN for high-dimensional gene expression data (Biomedical case study). Answer: In high-dimensional biomedical data, I would follow this robust pipeline:\n",
        "\n",
        "Standardization: Scale the data using StandardScaler as PCA and KNN are sensitive to variance.\n",
        "\n",
        "PCA for Dimensionality Reduction: Apply PCA to reduce thousands of genes into a few Principal Components.\n",
        "\n",
        "Choosing Components: Use a \"Scree Plot\" or cumulative explained variance (keeping 95% variance) to decide the number of components.\n",
        "\n",
        "KNN Classification: Train KNN on the reduced components to classify cancer types.\n",
        "\n",
        "Justification: This pipeline avoids overfitting (Curse of Dimensionality), removes noise, and speeds up the classification process, making it a reliable solution for complex medical data."
      ],
      "metadata": {
        "id": "-mfAoUPnVObR"
      }
    }
  ]
}